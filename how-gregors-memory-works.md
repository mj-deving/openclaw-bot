# How Gregor's Memory Works (ELI5)

## The Big Picture

```
  You write things            Gregor reads them later
  in markdown files           when you ask questions
       â”‚                              â–²
       â–¼                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    "index"    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    "search"    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  .md files  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚  Brain DB     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º   â”‚ Results â”‚
â”‚  (raw text) â”‚   chop up    â”‚  (SQLite)     â”‚   find best   â”‚ (top 6) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   + digest   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   matches      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Step 1: Writing Memories

Gregor's memory lives as plain markdown files:

```
~/.openclaw/workspace/memory/
â””â”€â”€ 2025-07-17.md          â—„â”€â”€ just a text file!
    â”‚
    â”‚  "My name is Gregor. I was created by Marius.
    â”‚   I engage on Lattice for the Demos protocol.
    â”‚   My key directives are..."
```

That's it. Plain text. You (or Gregor) just write `.md` files in that folder.

## Step 2: Indexing (The Meat Grinder)

When you run `openclaw memory index`, this happens:

```
    Your .md file (2992 bytes)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ "My name is Gregor. I was created by     â”‚
    â”‚ Marius. I engage on Lattice for the      â”‚
    â”‚ Demos protocol. My key directives are    â”‚
    â”‚ to be helpful, private, and accurate..." â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼  CHOP into chunks
                         (400 tokens each, 80 overlap)
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
              â”‚  Chunk 1  â”‚  Chunk 2  â”‚ ... â”‚  = 5 chunks total
              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
                    â”‚           â”‚
                    â–¼           â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   embeddinggemma-300m   â”‚  â—„â”€â”€ tiny AI brain (329MB)
              â”‚   (runs LOCALLY on VPS) â”‚      lives on your machine
              â”‚                        â”‚      NO data sent anywhere
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                    turns each chunk into
                    a list of 768 numbers
                          â”‚
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  [0.23, -0.41, 0.87,     â”‚  â—„â”€â”€ "embedding vector"
              â”‚   0.12, -0.55, 0.33,     â”‚      a fingerprint of
              â”‚   ... 768 numbers ...]   â”‚      what the text MEANS
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  ~/.openclaw/memory/     â”‚
              â”‚  main.sqlite             â”‚  â—„â”€â”€ all chunks + vectors
              â”‚                          â”‚      stored here
              â”‚  â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â” â”‚
              â”‚  â”‚ id â”‚ text   â”‚ vec   â”‚ â”‚
              â”‚  â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
              â”‚  â”‚ 1  â”‚ "My na â”‚ [0.2â€¦]â”‚ â”‚
              â”‚  â”‚ 2  â”‚ "I eng â”‚ [0.4â€¦]â”‚ â”‚
              â”‚  â”‚ 3  â”‚ "My ke â”‚ [-0.1â€¦â”‚ â”‚
              â”‚  â”‚ 4  â”‚ ...    â”‚ ...   â”‚ â”‚
              â”‚  â”‚ 5  â”‚ ...    â”‚ ...   â”‚ â”‚
              â”‚  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The key idea:** The 768 numbers capture the *meaning* of the text, not the words. "Lattice protocol" and "Demos network engagement" would have *similar* numbers even though they use different words.

## Step 3: Searching (The Magic Part)

When Gregor gets a question, two searches happen at once:

```
  Question: "What do you know about Lattice?"
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                       â–¼
   VECTOR SEARCH           TEXT SEARCH
   (meaning-based)         (word-based)
        â”‚                       â”‚
        â”‚  Turn question        â”‚  Just look for
        â”‚  into 768 numbers,    â”‚  the word "Lattice"
        â”‚  find chunks with     â”‚  in the text
        â”‚  similar numbers      â”‚
        â”‚                       â”‚
        â”‚  Score: 0.724         â”‚  Score: exact match
        â–¼                       â–¼
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼  COMBINE (hybrid)
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ 70% vector  â”‚  â—„â”€â”€ meaning matters more
              â”‚ 30% text    â”‚  â—„â”€â”€ but exact words help too
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼  then two more tricks:
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ MMR filter  â”‚  â—„â”€â”€ "don't repeat yourself"
              â”‚ (diversity) â”‚      picks DIFFERENT chunks,
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      not 5 copies of same thing
                     â”‚
                     â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Time decay   â”‚  â—„â”€â”€ newer memories rank higher
              â”‚ (30-day      â”‚      old stuff fades (but never
              â”‚  half-life)  â”‚      fully disappears)
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              Top 6 results (if score > 0.35)
              injected into Gregor's context
```

## The Whole Flow (End to End)

```
   You on Telegram: "What did Marius tell you about privacy?"
        â”‚
        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                 OpenClaw Gateway                  â”‚
   â”‚                                                   â”‚
   â”‚  1. Receive message from Telegram                 â”‚
   â”‚  2. Search memory â”€â”€â–º main.sqlite â”€â”€â–º 3 matches  â”‚
   â”‚  3. Build prompt:                                 â”‚
   â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
   â”‚     â”‚ System: "You are Gregor..."               â”‚â”‚
   â”‚     â”‚ Memory: [chunk about privacy directives]  â”‚â”‚  â—„â”€â”€ injected!
   â”‚     â”‚ Memory: [chunk about Marius identity]     â”‚â”‚
   â”‚     â”‚ Memory: [chunk about key rules]           â”‚â”‚
   â”‚     â”‚ User: "What did Marius tell you..."       â”‚â”‚
   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
   â”‚  4. Send to Claude Opus â”€â”€â–º get answer           â”‚
   â”‚  5. Reply on Telegram                             â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## TL;DR (Truly ELI5)

```
  ğŸ“ You write notes in files
       â†“
  ğŸ”ª Files get chopped into small pieces
       â†“
  ğŸ§  Tiny local AI turns each piece into a "meaning fingerprint"
       â†“
  ğŸ’¾ Fingerprints stored in a database
       â†“
  ğŸ” When Gregor gets a question, he finds pieces
     with the most similar meaning fingerprint
       â†“
  ğŸ’¬ Those pieces get stuffed into the prompt
     so Claude can answer with Gregor's memories
```

**What makes it special:** The fingerprints (embeddings) are made *locally* on the VPS by a tiny 329MB model. No text ever leaves the machine for memory search. Fully private.

## Technical Specs

| Component | Detail |
|-----------|--------|
| Database | `~/.openclaw/memory/main.sqlite` (SQLite + sqlite-vec) |
| Embedding model | `embeddinggemma-300m` (329MB GGUF, local via node-llama-cpp) |
| Vector dimensions | 768 |
| Chunk size | 400 tokens, 80 token overlap |
| Search type | Hybrid (70% vector + 30% FTS) |
| Reranking | MMR (lambda 0.7) for diversity |
| Recency boost | Temporal decay with 30-day half-life |
| Min score | 0.35 (below = filtered out) |
| Max results | 6 per query |
| Source files | `~/.openclaw/workspace/memory/*.md` |
